<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haokun Zhu</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haokun Zhu</name>
              </p>
             <p>Hi! I am a senior student from <strong>Shanghai Jiao Tong University (SJTU)</strong>.
                Currently, I am collaborating with <strong><a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Ph.D. Teng Hu</a></strong> at SJTU, under the supervision of <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>. Additionally, I am also receiving remote supervision from <strong><a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Prof. Yu-Kun Lai</a></strong> and <strong><a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Prof. Paul L. Rosin</a></strong>, who are based at Cardiff University.
                My research interests mainly lie in <strong>Computer Vision</strong> and <strong>Generative Models</strong>.
              </p>
              <p style="text-align:center">
                <a href="mailto: zhuhaokun@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/Resume_12_12.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=1MJRbuYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zwandering">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:60%;max-width:60%;margin-left:90%;text-align:right;">
              <a href="images/me_2.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/me_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am currently an undergraduate researcher both in Digital Media & Computer Vision Laboratory(<a href="https://dmcv.sjtu.edu.cn/">DMCV</a>) at SJTU on-site advised by <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>
                and remotely supervised by <strong><a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Prof. Yu-Kun Lai</a></strong> and
              <strong><a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Prof. Paul L. Rosin</a></strong> at Cardiff University.
                I'm interested in deep generative models like GANs and Diffusion Models. My dream is to develop generative models with strong capabilities and employ them in every-day life to bring convenience to everyone.
                <p style="text-align:justify">
                    In my past one year of research experience, I have explored a wide range of directions, including:
                    <li style="line-height:125%">
                      <strong>Few-shot Image Generation with Diffusion Model</strong>:
                      how to employ diffusion model in producing high-quality and diverse images in a new domain with only a small number of training data.
                    </li>
                    <li style="line-height:125%">
                      <strong>Aesthetic Guided Universal Style Transfer</strong>:
                      how to transfer the style of an arbitrary image to another content image while striking a balance among aesthetic qualities, style transfromation and content presevation.
                    </li>
                    <li style="line-height:125%">
                      <strong>Stroke-based Neural Painting</strong>:
                      how to recreate a pixel-based image with a set of brushstrokes like real human-beings while achieving both faithful reconstruction and stroke style at the same time.
                    </li>
                    <li style="line-height:125%">
                      <strong>Image Vectorization</strong>:
                      how to transform raster images into scalable vector graphics which have superior adaptability and detailed representation.
                    </li>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
              <strong>[2023.12.12]:</strong> Our paper <a href = "https://arxiv.org/abs/2311.05276">SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</a> is accepted by ICASSP 2024!
              </p>
              <p>
              <strong>[2023.11.09]:</strong> Our paper <a href = "https://arxiv.org/abs/2311.05276">SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</a> is on arXiv!
              </p>
              <p>
              <strong>[2023.07.26]:</strong> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766">Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</a> is accepted by ACM MM 2023!
              </p>
              <p>
              <strong>[2023.07.14]:</strong> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</a> is accepted by ICCV 2023!
              </p>
              
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications & Preprints (* means equal contribution)</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvpr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://zwandering.github.io/AUST.github.io/"><papertitle>AesStyler: Aesthetic Guided Universal Style Transfer</papertitle></a>
              </a>
              <br>
              <strong>Haokun Zhu</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <br>
              <em>Submitted to <a style="color:red" href="https://cvpr.thecvf.com/Conferences/2024/">CVPR 2024</a>, Under Review</em>
              <br>
              [<a href="https://zwandering.github.io/AUST.github.io/">website</a>]
              <br>
              <p></p>
              <p>We propose AesStyler, a novel Aesthetic Guided Universal Style Transfer method, which utilizes pre-trained aesthetiic assessment model, a novel Universal Aesthetic Codebook and a novel Universal and Specific Aesthetic-Guided Attention (USAesA) module. Extensive experiments and user-studies have demonstrated that our approach generates aesthetically more harmonious and pleasing results than the state-of-the-art methods.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icassp.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2311.05276"><papertitle>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</papertitle></a></papertitle>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://tsb0601.github.io/petertongsb/">Juang Ian Chong*</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://2024.ieeeicassp.org/">ICASSP 2024</a></em>
              <br>
               [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]
              <br>
              <p></p>
              <p>We propose SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector Graphics). Through a series of extensive experiments, we demonstrate that SAMVG can produce high quality SVGs in any domain while requiring less computation time and complexity compared to previous state-of-the-art methods.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mm.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766"><papertitle>Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</papertitle></a>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <strong>Haokun Zhu</strong>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://orcid.org/0009-0003-1887-6406/">Jinlong Peng</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://www.acmmm2023.org/">ACM MM 2023</a></em>
              <br>
              [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611766">pdf</a>] [<a href="https://github.com/sjtuplayer/Compositional_Neural_Painter">code</a>] [<a href="https://arxiv.org/abs/2309.03504">arXiv</a>]
              <br>
              <p></p>
              <p>We propose Compositional Neural Painter, a novel stroke-based rendering framework which dynamically predicts the next painting region based on the current canvas, instead of dividing the image plane uniformly into painting regions. Extensive experiments show our model outperforms the existing models in stroke-based neural painting.</p>
            </td>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html"><papertitle>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</papertitle></a>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://github.com/karrykkk">Siqi Kou</a>,
              <strong>Haokun Zhu</strong>,

              <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://iccv2023.thecvf.com/">ICCV 2023</a></em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.pdf">pdf</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_supplemental.pdf">supp</a>] [<a href="https://github.com/sjtuplayer/few-shot-diffusion">code</a>] [<a href="https://arxiv.org/abs/2309.03729">arXiv</a>]
              <br>
              <p></p>
              <p>We propose a novel phasic content fusing few-shot diffusion model with directional distribution consistency loss, which targets different learning objectives at distinct training stages of the diffusion model. Theoretical analysis, and experiments demonstrate the superiority of our approach in few-shot generative model adaption tasks.</p>
            </td>
          </tr>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Course Projects</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project1.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="data/4i_project.pdf"><papertitle>Image-to-Image Translation: From Line to Sketch</papertitle></a> <papertitle> (CV)</papertitle>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://github.com/NingLi670">Ning Li*</a>,
              <br>
              [<a href="data/4i_project.pdf">pdf</a>] [<a href="https://github.com/NingLi670/line-generate-sketch">code</a>]
              <br>
              <p></p>
              <p>This is the CS3511 course project. We use two frameworks, pix2pix and pixel2style2pixel, to solve an image-to-image translation task: line generation sketch task. Both methods achieve good performance in this task. We also achieved great results in <a href="https://codalab.lisn.upsaclay.fr/competitions/13111#results">the workshop of CGI-PSG2023</a> with this project, ranking 3rd in avg_FID and 2nd in avg_SSIM.</p>
            </td>
          </tr>
                    <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ray_tracing.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="data/ray_tracing.pdf"><papertitle>Real-time Ray Tracing with OpenGL</papertitle></a><papertitle> (CG)</papertitle>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://github.com/NingLi670">Ning Li*</a>
              <br>
              [<a href="data/ray_tracing.pdf">pdf</a>] [<a href="https://github.com/zwandering/RayTracing_OpenGL">code</a>]
              <br>
              <p></p>
              <p>This is the CS3310 Computer Graphics course project, which implements real-time ray tracing using OpenGL and use the graphics rendering pipeline to create visual effects like shadows, reflections, and refractions. It incorporates the SMAA algorithm for efficient anti-aliasing. Additionally, the project boosts ray tracing performance with algorithms such as Bounding Volume Hierarchy (BVH), ensuring the effective rendering.</p>
            </td>
          </tr>
                              <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/4_j.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="data/4_j.pdf"><papertitle>EEG-based Emotion Recognition</papertitle></a><papertitle> (Transfer Learning)</papertitle>
              </a>
              <br>
              <strong>Haokun Zhu</strong>,
              <br>
              [<a href="data/4_j.pdf">pdf</a>] [<a href="https://github.com/zwandering/4-J">code</a>]
              <br>
              <p></p>
              <p>This is the CS3507 course project. EEG-based emotion recognition is an important branch in the field of affective computing. In this project, I implement baselines, domain generalization and domain adpatation methods for the
EEG-based emotion recognition task. I implement abundant baseline models for the EEG-based emotion
recognition task, including SVM, MLP and ResNet. For domain generalization, I implement the Invariant
Risk Minimization domain generalization method. For domain adaptation, I implement 4 methods:
Transfer Component Analysis, Domain Adversarial Neural Network, Adversarial Discriminative Domain
Adaptation and Prototypical Representation based Pairwise Learning.
            </td>
          </tr>
                                        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bert.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="data/bert.pdf"><papertitle>Sentiment Analysis with Bert</papertitle></a><papertitle> (NLP)</papertitle>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://github.com/zhenghang1">Hang Zheng*</a>,
              <a> Quanling Yan</a>
              <br>
              [<a href="data/bert.pdf">pdf</a>] [<a href="https://github.com/zwandering/Bert_Sentiment_analysis">code</a>]
              <br>
              <p></p>
              <p>This is the CS3307 Internet Information Extraction course project. This project focuses on the Positive and Negative Sentiment Analysis, a binary sentiment analysis issue. Text sentiment analysis has a wide range of applications in fields such as social media and public opinion monitoring, including the analysis of positive and negative aspects of product reviews, monitoring of online corporate evaluations, etc. Therefore, achieving efficient and accurate sentiment analysis is of practical significance.</p>
            </td>
          </tr>
        </tbody>

        </table>

          <table class="sub-table" style="width: 200px;height: 100px;" align="center">
            <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=e7UZEtJzHSskg2uAm2OcvYFvUik7x45QuD4lLLv2j-E&cl=ffffff&w=a"></script>
          </table>
        </tbody>
        </table>



      </td>
    </tr>
  </table>
</body>

</html>
