<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Haokun Zhu</title>
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Haokun Zhu</name>
                  </p>
                  <p>
                    Hi! I am a second-year MSR student in the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> at <a href="http://www.cmu.edu/">Carnegie Mellon University</a>, advised by Prof. <a href="https://www.cs.cmu.edu/~./jeanoh/">Jean Oh</a> and Dr. <a href="https://frc.ri.cmu.edu/~zhangji/">Ji Zhang</a>. Prior to this, I pursued my undergraduate studies at <strong>Shanghai Jiao Tong University (SJTU)</strong>.
                    I collaborated with <strong><a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Ph.D. Teng Hu</a></strong> at SJTU, under the supervision of <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>, <strong><a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Prof. Yu-Kun Lai</a></strong> and <strong><a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Prof. Paul L. Rosin</a></strong> from Cardiff University. My research interests mainly lie in <strong>Embodied AI</strong>, <strong>Vision-Language Navigation</strong> and <strong>Object Navigation</strong>.
                  </p>
                  <p style="color:red">
                    <strong>I am actively seeking a PhD position for Fall 2026.</strong>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto: haokunz@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                    <a href="data/Resume_8_20.pdf">Resume</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=1MJRbuYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/zwandering">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:60%;max-width:60%;margin-left:90%;text-align:right;">
                  <a href="images/me_2.jpg">
                    <img style="width:75%;max-width:75%" alt="profile photo" src="images/me_2.jpg" class="hoverZoomLink">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research Section -->
          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I used to be an undergraduate researcher both in Digital Media & Computer Vision Laboratory(<a href="https://dmcv.sjtu.edu.cn/">DMCV</a>) at SJTU on-site advised by <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>
                    and remotely supervised by <strong><a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Prof. Yu-Kun Lai</a></strong> and
                    <strong><a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Prof. Paul L. Rosin</a></strong> at Cardiff University. I also interned in <strong><a href="https://open.youtu.qq.com/#/open">Youtu Lab</a></strong> at Tencent Technology (Shanghai) Co.Ltd, where I was advised by <strong><a href="https://pjl1995.github.io/">Jinlong Peng</a></strong>.
                    I'm interested in deep generative models like GANs and Diffusion Models. My dream is to develop generative models with strong capabilities and employ them in every-day life to bring convenience to everyone.
                  </p>
                  <p style="text-align:justify">
                    In my past one year of research experience, I have explored a wide range of directions, including:
                  </p>
                  <ul style="text-align:justify">
                    <li style="line-height:125%">
                      <strong>Few-shot Image Generation with Diffusion Model</strong>:
                      how to employ diffusion model in producing high-quality and diverse images in a new domain with only a small number of training data.
                    </li>
                    <li style="line-height:125%">
                      <strong>Aesthetic Guided Universal Style Transfer</strong>:
                      how to transfer the style of an arbitrary image to another content image while striking a balance among aesthetic qualities, style transfromation and content presevation.
                    </li>
                    <li style="line-height:125%">
                      <strong>Stroke-based Neural Painting</strong>:
                      how to recreate a pixel-based image with a set of brushstrokes like real human-beings while achieving both faithful reconstruction and stroke style at the same time.
                    </li>
                    <li style="line-height:125%">
                      <strong>Image Vectorization</strong>:
                      how to transform raster images into scalable vector graphics which have superior adaptability and detailed representation.
                    </li>
                    <li style="line-height:125%">
                      <strong>Multimodal Industrial Anomaly Detection</strong>:
                      how to address the issue of ineffective feature integration in 3D point cloud and RGB images and apply multimodality to enhance industrial anomaly detection.
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table> -->

          <!-- News Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                    <strong>[2025.11.21]:</strong> Our <a href="https://cmu-vln.github.io/">new demos on Object Navigation</a> is out now!
                  </p>
                  <p>
                    <strong>[2025.05.10]:</strong> Our paper <a href="https://zwandering.github.io/STRIVE.github.io/">STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation</a> is on arXiv!
                  </p>
                  <p>
                    <strong>[2024.07.15]:</strong> Our paper <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=1MJRbuYAAAAJ&citation_for_view=1MJRbuYAAAAJ:2osOgNQ5qMEC">AesStyler: Aesthetic Guided Universal Style Transfer</a> is accepted by ACM MM 2024!
                  </p>
                  <p>
                    <strong>[2023.12.12]:</strong> Our paper <a href="https://arxiv.org/abs/2311.05276">SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</a> is accepted by ICASSP 2024!
                  </p>
                  <p>
                    <strong>[2023.11.09]:</strong> Our paper <a href="https://arxiv.org/abs/2311.05276">SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</a> is on arXiv!
                  </p>
                  <p>
                    <strong>[2023.07.26]:</strong> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766">Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</a> is accepted by ACM MM 2023!
                  </p>
                  <p>
                    <strong>[2023.07.14]:</strong> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</a> is accepted by ICCV 2023!
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px 20px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td colspan="2">
                  <heading>Publications & Preprints (* means equal contribution, # means corresponding author)</heading>
                </td>
              </tr>
              <!-- Paper 1: Object Navigation -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/object_navigation.png" alt="object navigation" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://cmu-vln.github.io/">
                    <papertitle>Object Navigation Project</papertitle>
                  </a>
                  <br>
                  <strong>Haokun Zhu</strong>,
                  <a href="https://github.com/igzat1no">Zongtai Li</a>,
                  <a href="https://zihan-liu.replit.app/">Zihan Liu</a>,
                  <a href="https://sites.google.com/nyu.edu/kevinguos-profolio/">Kevin Guo</a>,
                  <a href="https://yuxin916.github.io/">Yuxin Cai</a>,
                  <a href="https://gfchen01.cc/">Guofei Chen</a>,
                  <a href="https://scholar.google.com/citations?user=UKVs2CEAAAAJ&hl=en">Chen Lv</a>
                  <a href="http://www.wangwenshan.com/">Wenshan Wang</a>
                  <a href="https://www.cs.cmu.edu/~jeanoh/">Jean Oh</a>
                  <a href="https://frc.ri.cmu.edu/~zhangji/">Ji Zhang</a>
                  <br>
                  <em>On Going Project</em>
                  <br>
                  [<a href="https://cmu-vln.github.io/">website</a>]
                  <br>
                  <p>We propose a cross-embodiment system supporting <strong>wheeled robots, quadrupeds(Unitree Go2), and humanoids(Unitree G1)</strong> that incrementally builds a multi-layer representation of the environment, including room, viewpoint, and object layers. Our system not only supports standard object navigation but also enables conditional navigation tasks, such as locating objects based on specific attributes or spatial relations. We conduct extensive real-world evaluations, including <strong>three long-range tests spanning an entire building floor</strong> and <strong>75 unit tests across multi-room environments</strong> (51 on wheeled robots, 18 on quadrupeds, and 6 on humanoids).</p>
                </td>
              </tr>
              <!-- Paper 1: STRIVE -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/strive.png" alt="STRIVE" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://zwandering.github.io/STRIVE.github.io/">
                    <papertitle>STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation</papertitle>
                  </a>
                  <br>
                  <strong>Haokun Zhu*</strong>,
                  <a href="https://github.com/igzat1no">Zongtai Li*</a>,
                  <a href="https://ariannaliu.github.io/">Zhixuan Liu</a>,
                  <a href="http://www.wangwenshan.com/">Wenshan Wang</a>,
                  <a href="https://frc.ri.cmu.edu/~zhangji/">Ji Zhang</a>,
                  <a href="https://jonfranc.com/">Jonathan Francis</a>,
                  <a href="https://www.cs.cmu.edu/~jeanoh/">Jean Oh</a>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <em><a style="color:red" href="https://2024.acmmm.org/">RSS 2025 Workshop</a> on Semantic Reasoning and Goal Understanding in Robotics</em>
                  <br>
                  [<a href="https://zwandering.github.io/STRIVE.github.io/">website</a>]
                  <br>
                  <p> We propose a novel framework that constructs a multi-layer representation of the environment during
                      navigation. Building on this representation, we propose a novel
                      two-stage navigation policy, integrating high-level planning guided by VLM rea-
                      soning with low-level VLM-assisted exploration to efficiently locate a goal object.
                      We evaluated our approach on three simulated benchmarks (HM3D, RoboTHOR,
                      and MP3D), and achieved state-of-the-art performance on both the success rate
                      (‚Üë 7.1%) and navigation efficiency (‚Üë 12.5%).</p>
                </td>
              </tr>

              <!-- Paper 1: MOSAIC -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/mosaic_out.png" alt="MOSAIC" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                    <a href="https://mosaic-cmubig.github.io/">
                      <papertitle>MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments</papertitle>
                    </a>
                  <br>
                    <a href="https://ariannaliu.github.io/">Zhixuan Liu</a>,
                    <strong>Haokun Zhu</strong>,
                    <a href="https://ruichen.pub/">Rui Chen</a>,
                    <a href="https://jonfranc.com/">Jonathan Francis</a>,
                    <a href="https://soonminhwang.github.io/">Soonmin Hwang</a>,
                    <a href="https://frc.ri.cmu.edu/~zhangji/">Ji Zhang</a>,
                    <a href="https://www.cs.cmu.edu/~./jeanoh/">Jean Oh</a>
                  <br>
                  <em><em>Accepted by <a style="color:red" href="https://2024.acmmm.org/">ICCV 2025</a></em></em>
                  <br>
                  [<a href="https://mosaic-cmubig.github.io/">website</a>] [<a href="https://arxiv.org/abs/2503.13816">Paper</a>] [<a href="https://mosaic-cmubig.github.io/">Code</a>]
                  <br>
                  <p> MOSAIC generates multi-view consistent images based on depth prior along robot navigation trajectories. It handles arbitrary    viewpoint changes in multi-room environments and generalizes to open vocabulary contexts.</p>
                </td>
              </tr>

              <!-- Paper 1: AesStyler -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cvpr.png" alt="AesStyler" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://zwandering.github.io/AUST.github.io/">
                    <papertitle>AesStyler: Aesthetic Guided Universal Style Transfer</papertitle>
                  </a>
                  <br>
                  <a href="https://yiranran.github.io/">Ran Yi*,#</a>,
                  <strong>Haokun Zhu*</strong>,
                  <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
                  <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
                  <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
                  <br>
                  <em>Accepted by <a style="color:red" href="https://2024.acmmm.org/">ACM MM 2024</a></em>
                  <br>
                  [<a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=1MJRbuYAAAAJ&citation_for_view=1MJRbuYAAAAJ:2osOgNQ5qMEC">website</a>]
                  <br>
                  <p>We propose AesStyler, a novel Aesthetic Guided Universal Style Transfer method, which utilizes pre-trained aesthetiic assessment model, a novel Universal Aesthetic Codebook and a novel Universal and Specific Aesthetic-Guided Attention (USAesA) module. Extensive experiments and user-studies have demonstrated that our approach generates aesthetically more harmonious and pleasing results than the state-of-the-art methods.</p>
                </td>
              </tr>

              <!-- Paper 2: SAMVG -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/icassp.png" alt="SAMVG" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2311.05276">
                    <papertitle>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</papertitle>
                  </a>
                  <br>
                  <strong>Haokun Zhu*</strong>,
                  <a href="https://tsb0601.github.io/petertongsb/">Juang Ian Chong*</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
                  <a href="https://yiranran.github.io/">Ran Yi</a>,
                  <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
                  <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
                  <br>
                  <em>Accepted by <a style="color:red" href="https://2024.ieeeicassp.org/">ICASSP 2024</a></em>
                  <br>
                  [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]
                  <br>
                  <p></p>
                  <p>We propose SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector Graphics). Through a series of extensive experiments, we demonstrate that SAMVG can produce high quality SVGs in any domain while requiring less computation time and complexity compared to previous state-of-the-art methods.</p>
                </td>
              </tr>

              <!-- Paper 3: M3DM-NR -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/tpami.png" alt="M3DM-NR" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2311.05276">
                    <papertitle>M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising</papertitle>
                  </a>
                  <br>
                  <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>,
                  <strong>Haokun Zhu</strong>,
                  <a href="https://orcid.org/0009-0003-1887-6406/">Jinlong Peng</a>,
                  <a href="">Yue Wang</a>,
                  <a href="https://yiranran.github.io/">Ran Yi</a>,
                  <a href="">Yunsheng Wu</a>,
                  <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>,
                  <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>
                  <br>
                  <em>Accepted by <a style="color:red" href="https://2024.acmmm.org/">TPAMI</a></em>
                  <br>
                  [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]
                  <br>
                  <p>We propose M3DM-NR, a novel noise-resistant framework to leverage the strong multi-modal(image and point cloud) discriminative capabilities of CLIP. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection</p>
                </td>
              </tr>

              <!-- Paper 4: Stroke-based Neural Painting -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/mm.png" alt="Neural Painting" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766">
                    <papertitle>Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
                  <a href="https://yiranran.github.io/">Ran Yi</a>,
                  <strong>Haokun Zhu</strong>,
                  <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
                  <a href="https://orcid.org/0009-0003-1887-6406/">Jinlong Peng</a>,
                  <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
                  <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>,
                  <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
                  <br>
                  <em>Accepted by <a style="color:red" href="https://www.acmmm2023.org/">ACM MM 2023</a></em>
                  <br>
                  [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611766">pdf</a>] [<a href="https://github.com/sjtuplayer/Compositional_Neural_Painter">code</a>] [<a href="https://arxiv.org/abs/2309.03504">arXiv</a>]
                  <br>
                  <p>We propose Compositional Neural Painter, a novel stroke-based rendering framework which dynamically predicts the next painting region based on the current canvas, instead of dividing the image plane uniformly into painting regions. Extensive experiments show our model outperforms the existing models in stroke-based neural painting.</p>
                </td>
              </tr>

              <!-- Paper 5: Phasic Content Fusing -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/iccv.png" alt="Phasic Content" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">
                    <papertitle>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
                  <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
                  <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
                  <a href="https://yiranran.github.io/">Ran Yi</a>,
                  <a href="https://github.com/karrykkk">Siqi Kou</a>,
                  <strong>Haokun Zhu</strong>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a>,
                  <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
                  <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>,
                  <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
                  <br>
                  <em>Accepted by <a style="color:red" href="https://iccv2023.thecvf.com/">ICCV 2023</a></em>
                  <br>
                  [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.pdf">pdf</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_supplemental.pdf">supp</a>] [<a href="https://github.com/sjtuplayer/few-shot-diffusion">code</a>] [<a href="https://arxiv.org/abs/2309.03729">arXiv</a>]
                  <br>
                  <p>We propose a novel phasic content fusing few-shot diffusion model with directional distribution consistency loss, which targets different learning objectives at distinct training stages of the diffusion model. Theoretical analysis, and experiments demonstrate the superiority of our approach in few-shot generative model adaption tasks.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Visitor Counter -->
          <table class="sub-table" style="width:200px;height:100px;" align="center">
            <tbody>
              <tr>
                <td>
                  <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=e7UZEtJzHSskg2uAm2OcvYFvUik7x45QuD4lLLv2j-E&cl=ffffff&w=a"></script>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
